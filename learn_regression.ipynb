{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from eventcnn.datasources.davis import DavisDataset\n",
    "from eventcnn.datasources.eventsource import EventSource\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset1 = DavisDataset.named_dataset(\"poster_translation\")\n",
    "dataset2 = DavisDataset.named_dataset(\"shapes_translation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source1 = EventSource(dataset1, \n",
    "                      testing_fraction=0.2,\n",
    "                      validation_fraction=0.4,\n",
    "                      events_per_block=10000)\n",
    "source2 = EventSource(dataset2, \n",
    "                      testing_fraction=0.2,\n",
    "                      validation_fraction=0.4,\n",
    "                      events_per_block=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The + operator combines event sources, respecting their \n",
    "# individual training/test/validation splits\n",
    "combined = source1 + source2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the clusters like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120, 90, 4]\n"
     ]
    }
   ],
   "source": [
    "elem = combined.training(shuffle=True).__next__()\n",
    "sparse_tensor1 = elem.events_as_sparse_tensor_rescaled( n_layers = 5, scaling = 2)\n",
    "print(sparse_tensor1.shape)\n",
    "dense_tensor1 = tf.sparse_tensor_to_dense(sparse_tensor1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<eventcnn.datasources.eventblock.EventBlock at 0x7f989462ada0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elem.delta_position\n",
    "combined.testing().__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_batch(generator, batch_size):\n",
    "    list_of_tensors = []\n",
    "    list_of_truth = []\n",
    "    num_blocks = 0\n",
    "    for block in generator:\n",
    "        num_blocks += 1\n",
    "        if num_blocks > batch_size:\n",
    "            break\n",
    "        list_of_tensors.append(tf.sparse_tensor_to_dense(block.events_as_sparse_tensor_rescaled( n_layers = 5, scaling = 4)))\n",
    "        list_of_truth.append(block.delta_position)\n",
    "    X = tf.pack(list_of_tensors)\n",
    "    y = tf.pack(list_of_truth)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = get_data_batch(combined.testing(), 50)\n",
    "# X_validation, y_validation = get_data_batch(combined.validation(), 5) TODO: Somehow does not work?!?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"pack_3:0\", shape=(500, 60, 45, 4), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46.661661000000002, 46.666632999999997]\n",
      "[38.087312001000001, 38.106126001]\n",
      "[21.872201, 21.875403000999999]\n",
      "[8.5099020000000003, 8.5233780010000011]\n",
      "[59.509162001000007, 59.511696001000004]\n",
      "[2.3797550009999999, 2.3878360010000002]\n",
      "[0.36012300000000003, 0.37505699999999997]\n",
      "[25.028206000000001, 25.032253999999998]\n",
      "[59.289725001000001, 59.292600999999998]\n",
      "[53.723846000000002, 53.727438001000003]\n"
     ]
    }
   ],
   "source": [
    "num_blocks = 0\n",
    "for block in combined.training(shuffle=True):\n",
    "    num_blocks += 1\n",
    "    if num_blocks > 10:\n",
    "        break\n",
    "    print(block.time_span)\n",
    "    sparse_tensor = block.events_as_sparse_tensor_rescaled( n_layers = 5, scaling = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll bundle groups of examples during training for efficiency.\n",
    "# This defines the size of the batch.\n",
    "BATCH_SIZE = 60\n",
    "\n",
    "# We aggregate our data to 5 time slices\n",
    "N_TIME_SLICES = 5\n",
    "\n",
    "# Inversely scaling the input image resolution\n",
    "SCALING_FAC = 2\n",
    "\n",
    "# Number of observed coordinates\n",
    "NUM_LABELS = 3\n",
    "\n",
    "# Resolution\n",
    "X_RES = np.uint32(240 / SCALING_FAC)\n",
    "Y_RES = np.uint32(180 / SCALING_FAC)\n",
    "\n",
    "# The random seed that defines initialization.\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# This is where training samples and labels are fed to the graph.\n",
    "# These placeholder nodes will be fed a batch of training data at each\n",
    "# training step, which we'll write once we define the graph structure.\n",
    "train_data_node = tf.placeholder(  tf.float32,\n",
    "                                   shape=(BATCH_SIZE, Y_RES, X_RES, N_TIME_SLICES))\n",
    "train_labels_node = tf.placeholder(tf.float32,\n",
    "                                   shape=(BATCH_SIZE, NUM_LABELS))\n",
    "\n",
    "# For the validation and test data, we'll just hold the entire dataset in\n",
    "# one constant node.\n",
    "# validation_data_node = tf.constant(combined.validation())\n",
    "# test_data_node = tf.constant(X_test)\n",
    "\n",
    "# The variables below hold all the trainable weights. For each, the\n",
    "# parameter defines how the variables will be initialized.\n",
    "\n",
    "# 5x5x5 filter, depth 32.\n",
    "conv1_weights = tf.Variable(\n",
    "  tf.truncated_normal([5, 5, 5, N_TIME_SLICES, 32],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv1_biases = tf.Variable(tf.zeros([32]))\n",
    "\n",
    "conv2_weights = tf.Variable(\n",
    "  tf.truncated_normal([5, 5, 5, 32, 64],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv2_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "\n",
    "# fully connected, depth 512.\n",
    "fc1_weights = tf.Variable(  \n",
    "  tf.truncated_normal([Y_RES // 4 * X_RES // 4 * 64, 512],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "\n",
    "# fully connected, depth 512.\n",
    "fc2_weights = tf.Variable(\n",
    "  tf.truncated_normal([512, NUM_LABELS],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc2_biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS]))\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(data, train=False):\n",
    "    \"\"\"The Model definition.\"\"\"\n",
    "    # 3D convolution, with 'SAME' padding (i.e. the output feature map has\n",
    "    # the same size as the input). Note that {strides} is a 5D array whose\n",
    "    # shape matches the data layout: [tensor_index, y, x, time_layer].\n",
    "    conv = tf.nn.conv3d(data,\n",
    "                        conv1_weights,\n",
    "                        strides=[1, 1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "\n",
    "    # Bias and rectified linear non-linearity.\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "\n",
    "    # Max pooling. The kernel size spec ksize also follows the layout of\n",
    "    # the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "    pool = tf.nn.max_pool3d(relu,\n",
    "                          ksize=[1, 2, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    \n",
    "    \n",
    "    conv = tf.nn.conv3d(pool,\n",
    "                        conv2_weights,\n",
    "                        strides=[1, 1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "    pool = tf.nn.max_pool3d(relu,\n",
    "                          ksize=[1, 2, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "    # Reshape the feature map cuboid into a 3D matrix to feed it to the\n",
    "    # fully connected layers.\n",
    "    pool_shape = pool.get_shape().as_list()\n",
    "    reshape = tf.reshape(\n",
    "        pool,\n",
    "        [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3] * pool_shape[4]])\n",
    "  \n",
    "    # Fully connected layer. Note that the '+' operation automatically\n",
    "    # broadcasts the biases.\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "\n",
    "    # Add a 50% dropout during training only. Dropout also scales\n",
    "    # activations such that no rescaling is needed at evaluation time.\n",
    "    if train:\n",
    "        hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "    return tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training computation: logits + cross-entropy loss.\n",
    "logits = model(train_data_node, True)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "  logits, train_labels_node))\n",
    "\n",
    "# L2 regularization for the fully connected parameters.\n",
    "regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "# Add the regularization term to the loss.\n",
    "loss += 5e-4 * regularizers\n",
    "\n",
    "# Optimizer: set up a variable that's incremented once per batch and\n",
    "# controls the learning rate decay.\n",
    "batch = tf.Variable(0)\n",
    "# Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "  0.01,                # Base learning rate.\n",
    "  batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "  train_size,          # Decay step.\n",
    "  0.95,                # Decay rate.\n",
    "  staircase=True)\n",
    "# Use simple momentum for the optimization.\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(loss, global_step=batch)\n",
    "\n",
    "# Predictions for the minibatch, validation set and test set.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "# We'll compute them only once in a while by calling their {eval()} method.\n",
    "validation_prediction = tf.nn.softmax(model(validation_data_node))\n",
    "test_prediction = tf.nn.softmax(model(test_data_node))\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new interactive session that we'll use in\n",
    "# subsequent code cells.\n",
    "s = tf.InteractiveSession()\n",
    "\n",
    "# Use our newly created session as the default for \n",
    "# subsequent operations.\n",
    "s.as_default()\n",
    "\n",
    "# Initialize all the variables we defined above.\n",
    "tf.initialize_all_variables().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 60\n",
    "\n",
    "# Grab the first BATCH_SIZE examples and labels.\n",
    "batch_data = train_data[:BATCH_SIZE, :, :, :]\n",
    "batch_labels = train_labels[:BATCH_SIZE]\n",
    "\n",
    "# This dictionary maps the batch data (as a numpy array) to the\n",
    "# node in the graph it should be fed to.\n",
    "feed_dict = {train_data_node: batch_data,\n",
    "             train_labels_node: batch_labels}\n",
    "\n",
    "# Run the graph and fetch some of the nodes.\n",
    "_, l, lr, predictions = s.run(\n",
    "  [optimizer, loss, learning_rate, train_prediction],\n",
    "  feed_dict=feed_dict)\n",
    "\n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}